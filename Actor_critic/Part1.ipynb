{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4d8c9b006ada45",
   "metadata": {},
   "source": [
    "## <center>CSE 546: Reinforcement Learning</center>\n",
    "### <center>Prof. Alina Vereshchaka</center>\n",
    "#### <center>Spring 2025</center>\n",
    "\n",
    "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a6d891e2fb312",
   "metadata": {},
   "source": [
    "## Section 0: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53473293aa9daf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108695e90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d9c34ff222994",
   "metadata": {},
   "source": [
    "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
    "\n",
    "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
    "- A. Completely separate actor and critic networks\n",
    "- B. A shared network with two output heads\n",
    "\n",
    "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fa7887dd4f858",
   "metadata": {},
   "source": [
    "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
    "\n",
    "Define a class `SeparateActorCritic`. Your goal is to:\n",
    "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
    "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
    "- The critic should output a single scalar value.\n",
    "\n",
    " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
    "\n",
    "```python\n",
    "# TODO: Define SeparateActorCritic class\n",
    "```\n",
    "\n",
    " Next, simulate training using dummy tensors:\n",
    "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
    "2. Compute the actor loss using the advantage (return - value).\n",
    "3. Compute the critic loss as mean squared error between values and returns.\n",
    "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
    "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
    "\n",
    "```python\n",
    "# TODO: Simulate loss computation and backpropagation\n",
    "```\n",
    "\n",
    "ðŸ”— Helpful references:\n",
    "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd6b81ed1791e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
    "import torch.optim as optim\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "class SeparateActorCritic(nn.Module):\n",
    "    def __init__(self, inputLayer,numOfActions, hidden=128):\n",
    "        super().__init__()\n",
    "        #Actor Network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(inputLayer,hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden,numOfActions),\n",
    "            nn.Softmax(dim=-1) #Softmax as output\n",
    "        )\n",
    "        #critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(inputLayer,hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden,1) #Single scalar value as output\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        possible_actions=self.actor(x)\n",
    "        criticValue = self.critic(x)\n",
    "        return possible_actions,criticValue\n",
    "# END_YOUR_CODE\n",
    "\n",
    "#Simulate loss computation and Backpropogation\n",
    "\n",
    "batchSize = 3\n",
    "inputLayer =10\n",
    "numOfActions = 4\n",
    "net = SeparateActorCritic(inputLayer,numOfActions)\n",
    "\n",
    "observations = torch.randn(batchSize,inputLayer)\n",
    "\n",
    "returns = torch.randn(batchSize, requires_grad=True)\n",
    "values = torch.randn(batchSize, requires_grad=True)\n",
    "entropies = torch.randn(batchSize, requires_grad=True)\n",
    "logProbabilities = torch.randn(batchSize,requires_grad=True)\n",
    "\n",
    "#Advantage \n",
    "advantage = returns-values.detach()\n",
    "\n",
    "#Actor loss\n",
    "actorLoss = -(logProbabilities * advantage).mean()-0.01 * entropies.mean()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#Critic loss\n",
    "criticLoss = criterion(values,returns)\n",
    "\n",
    "#Single optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "totalLoss = actorLoss +criticLoss\n",
    "optimizer.zero_grad()\n",
    "totalLoss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "#Seperate Optimizers\n",
    "#model = SeparateActorCritic(inputLayer,numOfActions)\n",
    "actorOpt = optim.Adam(net.actor.parameters(),lr=0.001)\n",
    "criticOpt = optim.Adam(net.critic.parameters(),lr =0.001)\n",
    "\n",
    "#Actor's step\n",
    "actorOpt.zero_grad()\n",
    "actorLoss.backward(retain_graph=True)\n",
    "actorOpt.step()\n",
    "\n",
    "#Critic's step\n",
    "criticOpt.zero_grad(set_to_none=True)\n",
    "criticLoss.backward()\n",
    "criticOpt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e90c88108cd2e",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: This is a classic implementation where we separate Actor and Critic networks. Actor network gives the probabilities over actions, and Critic gives the value -  here we have Asymmetric network for Actor and Critic.\n",
    "This is prefered in Continuous tasks and helps with better flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751976b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64081a606b93029d",
   "metadata": {},
   "source": [
    "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
    "\n",
    "Now define a class `SharedActorCritic`:\n",
    "- Build a shared base network (e.g., linear layer + ReLU)\n",
    "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
    "\n",
    "```python\n",
    "# TODO: Define SharedActorCritic class\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
    "2. Simulate dummy rewards and compute advantage.\n",
    "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
    "\n",
    "```python\n",
    "# TODO: Simulate shared network loss computation and backpropagation\n",
    "```\n",
    "\n",
    " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
    "\n",
    "ðŸ”— More reading:\n",
    "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
    "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
    "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a48f882fff11aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class SharedActorCritic(nn.Module):\n",
    "  def __init__(self,inputLayer,actionSpace,hiddenLayer=128):\n",
    "    super().__init__()\n",
    "    self.sharedBaseLayers = nn.Sequential(\n",
    "      nn.Linear(inputLayer,hiddenLayer),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.isContinuous = isinstance(actionSpace, gym.spaces.Box)\n",
    "    if self.isContinuous:\n",
    "      actionDimenstion = actionSpace.shape[0]\n",
    "      self.actorMean = nn.Linear(hiddenLayer,actionDimenstion)\n",
    "      self.actorLogStd = nn.Parameter(\n",
    "        torch.zeros(\n",
    "          actionDimenstion\n",
    "        )\n",
    "      )\n",
    "    else:\n",
    "      self.actorHead = nn.Sequential(\n",
    "        nn.Linear(\n",
    "          hiddenLayer,\n",
    "          actionSpace.n\n",
    "        ),\n",
    "        nn.Softmax(dim=-1)\n",
    "      )\n",
    "    #Actor's head \n",
    "    # self.actorHead = nn.Sequential(\n",
    "    #   nn.Linear(hiddenLayer,numOfActions),\n",
    "    #   nn.Softmax(dim=-1)\n",
    "    # )\n",
    "    self.criticHead = nn.Linear(hiddenLayer,1) #output state value\n",
    "\n",
    "  def forward(self,x):\n",
    "    sharedOutput = self.sharedBaseLayers(x)\n",
    "    if self.isContinuous:\n",
    "      mean = self.actorMean(sharedOutput)\n",
    "      std = self.actorLogStd.exp().expand_as(mean)\n",
    "      return mean,std,self.criticHead(sharedOutput)\n",
    "    else:\n",
    "      actionProbailities = self.actorHead(sharedOutput)\n",
    "      stateValue = self.criticHead(sharedOutput)\n",
    "      return actionProbailities,stateValue\n",
    "\n",
    "#Simulate loss computation and Backpropogation\n",
    "\n",
    "batchSize = 3\n",
    "inputLayer =10\n",
    "#numOfActions = 4\n",
    "actionSpace = gym.spaces.Discrete(4)\n",
    "net = SharedActorCritic(inputLayer,actionSpace= actionSpace)\n",
    "\n",
    "observations = torch.randn(batchSize,inputLayer)\n",
    "actionProb , stateValues = net(observations)\n",
    "\n",
    "returns = torch.randn(batchSize, requires_grad=True)\n",
    "values = torch.randn(batchSize, requires_grad=True)\n",
    "entropies = torch.randn(batchSize, requires_grad=True)\n",
    "logProbabilities = torch.randn(batchSize,requires_grad=True)\n",
    "\n",
    "#Advantage \n",
    "advantage = returns-values.detach()\n",
    "\n",
    "#Actor loss\n",
    "actorLoss = -(logProbabilities * advantage).mean()-0.01 * entropies.mean()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#Critic loss\n",
    "criticLoss = criterion(values,returns)\n",
    "totalLoss = actorLoss +criticLoss\n",
    "\n",
    "#Single optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "totalLoss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974e302d1fdb028",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: It is primarily preferred in environment with high dimensional observations, here we have a shared base network for input and output is branched to 2 heads - Actor(Policy o/p) and Critic(Value function).\n",
    "It is preferred when we have high precision requiring tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad5ea9406f8b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb645eb009b85b1c",
   "metadata": {},
   "source": [
    "## Section 2: Auto-Adaptive Network Setup for Environments\n",
    "\n",
    "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223b6ddf43abee5",
   "metadata": {},
   "source": [
    "### Task 2: Auto-generate Input and Output Layers\n",
    "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
    "- The input layer should match the environment's observation space.\n",
    "- The output layer for the **actor** should depend on the action space:\n",
    "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
    "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
    "- The **critic** always outputs a single scalar value.\n",
    "\n",
    "```python\n",
    "# TODO: Define function `create_shared_network(env)`\n",
    "```\n",
    "\n",
    "#### Environments to Support:\n",
    "Test your function with the following environments:\n",
    "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
    "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
    "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
    "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
    "\n",
    "```python\n",
    "# TODO: Loop through environments and test `create_shared_network`\n",
    "```\n",
    "\n",
    "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
    "\n",
    "ðŸ”— Observation/Action Space Docs:\n",
    "- https://gymnasium.farama.org/api/spaces/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6d249ff9277403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on varied environments\n",
      "Testing for CliffWalking-v0\n",
      "Observation is an integer, not an array.\n",
      "Network Architecture: \n",
      "SharedActorCritic(\n",
      "  (sharedBaseLayers): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (actorHead): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=4, bias=True)\n",
      "    (1): Softmax(dim=-1)\n",
      "  )\n",
      "  (criticHead): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Testing for lunarlander-v3\n",
      "Observation shape: (8,)\n",
      "Network Architecture: \n",
      "SharedActorCritic(\n",
      "  (sharedBaseLayers): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (actorHead): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=4, bias=True)\n",
      "    (1): Softmax(dim=-1)\n",
      "  )\n",
      "  (criticHead): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Testing for PongNoFrameskip-v4\n",
      "Observation shape: (210, 160, 3)\n",
      "Network Architecture: \n",
      "SharedActorCritic(\n",
      "  (sharedBaseLayers): Sequential(\n",
      "    (0): Linear(in_features=100800, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (actorHead): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (1): Softmax(dim=-1)\n",
      "  )\n",
      "  (criticHead): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Testing for HalfCheetah-v5\n",
      "Observation shape: (17,)\n",
      "Network Architecture: \n",
      "SharedActorCritic(\n",
      "  (sharedBaseLayers): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (actorMean): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (criticHead): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import ale_py\n",
    "#import nn.functional as F\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "def create_shared_network(env, hiddenLayer=128):\n",
    "  actionSpace= env.action_space \n",
    "  observationSpace = env.observation_space\n",
    "  \n",
    "  #Check input dimensions \n",
    "  if isinstance(observationSpace, gym.spaces.Discrete): inputLayer = observationSpace.n #one hot encoding\n",
    "  elif isinstance(observationSpace, gym.spaces.Box): inputLayer = int(np.prod(observationSpace.shape))\n",
    "  else: raise ValueError(f\"Environment not supported\")\n",
    "\n",
    "  return SharedActorCritic(inputLayer,actionSpace)\n",
    "# END_YOUR_CODE\n",
    "#preprcessing the environment\n",
    "def preprocessObservation(obs, observationSpace):\n",
    "  if isinstance(observationSpace, gym.spaces.Discrete):\n",
    "    oneHotEncoded = np.zeros(observationSpace.n, dtype=np.float32)\n",
    "    oneHotEncoded[obs] = 1.0\n",
    "    return torch.tensor(oneHotEncoded).float().unsqueeze(0)\n",
    "  elif isinstance(observationSpace, gym.spaces.Box):\n",
    "    flattenedObs = np.array(obs).flatten()\n",
    "    return torch.tensor(flattenedObs,dtype=torch.float32).unsqueeze(0)\n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported Environment: {observationSpace}\")\n",
    "  \n",
    "#Testing on different envs\n",
    "gym.register_envs(ale_py)\n",
    "environments = {\n",
    "  \"CliffWalking-v0\" : gym.make(\"CliffWalking-v0\"),\n",
    "  \"lunarlander-v3\": gym.make(\"LunarLander-v3\"),\n",
    "  \"PongNoFrameskip-v4\": gym.make(\"PongNoFrameskip-v4\"),\n",
    "  #\"PongNoFrameskip-v4\": AtariPreprocessing(gym.make(\"PongNoFrameskip-v4\")),\n",
    "  \"HalfCheetah-v5\": gym.make(\"HalfCheetah-v5\"),\n",
    "}\n",
    "\n",
    "print(\"Testing on varied environments\")\n",
    "for environmentName, environment in environments.items():\n",
    "  print(f\"Testing for {environmentName}\")\n",
    "  net = create_shared_network(environment)\n",
    "  #print(sharedNetwork)\n",
    "  observation, _ = environment.reset()\n",
    "  observationTensor = preprocessObservation(observation, environment.observation_space)\n",
    "  \n",
    "  #print(f\"Observation shape: {observation.shape}\")\n",
    "  if isinstance(observation, tuple):\n",
    "    observation = observation[0]\n",
    "  if isinstance(observation, int):\n",
    "    print(\"Observation is an integer, not an array.\")\n",
    "  else:\n",
    "    print(f\"Observation shape: {observation.shape}\")\n",
    "  print(f\"Network Architecture: \\n{net}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd13f0b62b30ff",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: Here we try to create a Shared A-C network such that it can adapt to different environments - both continuous and discrete. Our model is flexible and is then tested on multiple environments.\n",
    "We check different observation space and then generate input layer based on the environment  - It is more aligned towards developing Universal RL model that can handle multiple env types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c886fa536a639",
   "metadata": {},
   "source": [
    "### Task 3: Write Observation Normalization Function\n",
    "Create a function `normalize_observation(obs, env)` that:\n",
    "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
    "- If so, normalize the input observation.\n",
    "- Otherwise, return the observation unchanged.\n",
    "\n",
    "```python\n",
    "# TODO: Define `normalize_observation(obs, env)`\n",
    "```\n",
    "\n",
    "Test this function with observations from:\n",
    "- `LunarLander-v3`\n",
    "- `PongNoFrameskip-v4`\n",
    "\n",
    "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc7ee06112cf7d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original  Lunar Lander Observation: [ 4.2343141e-05  1.4182791e+00  4.2768968e-03  3.2705724e-01\n",
      " -4.2300217e-05 -9.6876046e-04  0.0000000e+00  0.0000000e+00]\n",
      "Lunar Lander Normalized:[0.50000846 0.7836558  0.50021386 0.51635283 0.49999663 0.49995154\n",
      " 0.         0.        ]\n",
      "Original Pong Observation: [[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]]\n",
      "\n",
      " [[109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  ...\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]]\n",
      "\n",
      " [[109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  ...\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  ...\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]]\n",
      "\n",
      " [[ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  ...\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]]\n",
      "\n",
      " [[ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  ...\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]]]\n",
      "Pong Normalized:[[[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]]\n",
      "\n",
      " [[0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  ...\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]]\n",
      "\n",
      " [[0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  ...\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]\n",
      "  [0.42745098 0.4627451  0.16862745]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  ...\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]]\n",
      "\n",
      " [[0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  ...\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]]\n",
      "\n",
      " [[0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  ...\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]\n",
      "  [0.20784314 0.37254902 0.09411765]]]\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import ale_py\n",
    "\n",
    "\n",
    "def normalize_observation(obs,env):\n",
    "  observationSpace = env.observation_space\n",
    "  \n",
    "  if isinstance(observationSpace,gym.spaces.Box): #check observation space type\n",
    "    #for Atari\n",
    "    if np.issubdtype(np.integer,obs.dtype): return obs.astype(np.float32)/255.0\n",
    "    low = observationSpace.low\n",
    "    high = observationSpace.high\n",
    "    #using min-max normalization formula\n",
    "    return (obs -low)/(high-low)\n",
    "  \n",
    "  return obs\n",
    "\n",
    "#Testing \n",
    "lunarlanderEnv = gym.make(\"LunarLander-v3\")\n",
    "observation1, _ = lunarlanderEnv.reset()\n",
    "normalizedObservation = normalize_observation(observation1, lunarlanderEnv)\n",
    "print(f\"Original  Lunar Lander Observation: {observation1}\")\n",
    "print(f\"Lunar Lander Normalized:{normalizedObservation}\")\n",
    "\n",
    "#Testing\n",
    "#PongEnv\n",
    "gym.register_envs(ale_py)\n",
    "pongEnv = gym.make(\"PongNoFrameskip-v4\")\n",
    "observation2, _ = pongEnv.reset()\n",
    "normalizedObservation = normalize_observation(observation2, pongEnv)\n",
    "print(f\"Original Pong Observation: {observation2}\")\n",
    "print(f\"Pong Normalized:{normalizedObservation}\")\n",
    "\n",
    "  \n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ed2a6e7ca7a7b",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: We normalize the observations from each of the environments.\n",
    "This is preferred so that our Agent can receive standard input, and we can have consistent training and convergence as well.\n",
    "We apply min max normalization over continuous data and normalize pixel data to have better understanding of features and avoid instability due to high contrast regions or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78211b617a843f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b5fb5353307f514",
   "metadata": {},
   "source": [
    "## Section 4: Gradient Clipping\n",
    "\n",
    "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
    "\n",
    "### Task 4: Clip Gradients for Actor-Critic Networks\n",
    "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
    "```python\n",
    "# During training, after loss.backward():\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "```\n",
    "\n",
    "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
    "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
    "\n",
    "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7327507fb6e803ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Before Clipping-----------------\n",
      "Parameter: torch.Size([128, 10]), Gradient: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0129,  0.0087,  0.0187,  ..., -0.0058, -0.0001,  0.0034],\n",
      "        [-0.0058,  0.0033,  0.0116,  ..., -0.0040, -0.0064,  0.0028],\n",
      "        ...,\n",
      "        [ 0.0039, -0.0025, -0.0060,  ...,  0.0019,  0.0009, -0.0012],\n",
      "        [ 0.0003, -0.0006,  0.0015,  ..., -0.0007, -0.0037,  0.0007],\n",
      "        [-0.0125,  0.0079,  0.0190,  ..., -0.0064, -0.0029,  0.0036]])\n",
      "Parameter: torch.Size([128]), Gradient: tensor([ 0.0000e+00,  8.8644e-03,  7.2258e-03,  1.7258e-02, -1.3295e-02,\n",
      "         9.6985e-03,  2.8820e-03,  9.5387e-04,  5.0316e-05, -4.6311e-04,\n",
      "         3.7699e-04,  1.2406e-03,  0.0000e+00,  1.8069e-03, -4.0512e-03,\n",
      "        -3.8925e-03, -3.7481e-03, -1.3704e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -9.6184e-03,  0.0000e+00, -1.6217e-02, -4.6188e-04, -9.1634e-05,\n",
      "        -6.6576e-03,  0.0000e+00,  0.0000e+00,  7.3199e-03,  0.0000e+00,\n",
      "        -1.1983e-02, -3.1851e-03,  3.0568e-03, -6.9277e-04,  1.2080e-03,\n",
      "        -1.3257e-03, -4.8956e-03,  1.2640e-03, -5.0377e-03,  0.0000e+00,\n",
      "        -7.4145e-03, -3.6153e-03,  0.0000e+00,  2.8497e-03,  1.6296e-03,\n",
      "         8.5162e-04, -3.0803e-03,  1.7514e-02,  0.0000e+00,  2.0063e-04,\n",
      "        -1.2860e-02,  0.0000e+00,  1.3858e-02,  2.8430e-04,  5.6074e-04,\n",
      "        -6.8820e-03,  4.8913e-03,  0.0000e+00, -1.6762e-02, -4.5965e-03,\n",
      "        -6.1517e-03,  1.4468e-02, -5.9723e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -7.4546e-03, -1.3959e-02,  0.0000e+00,  6.1053e-03, -1.0314e-02,\n",
      "        -4.3505e-03, -3.6072e-03, -4.6453e-03, -4.2216e-03,  0.0000e+00,\n",
      "         4.4427e-03,  0.0000e+00,  2.7247e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -3.6113e-03,  5.0171e-03, -6.5924e-03,  1.1473e-02,\n",
      "        -7.1127e-05,  0.0000e+00, -3.8783e-03, -6.9353e-03,  4.5657e-04,\n",
      "        -2.7216e-03,  4.3771e-04, -2.4485e-04,  5.4758e-03,  0.0000e+00,\n",
      "         0.0000e+00,  4.7680e-03,  0.0000e+00, -3.3854e-03, -7.9903e-03,\n",
      "         1.1756e-02, -2.3904e-03,  1.3052e-02, -2.3462e-03,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  5.3158e-04,  0.0000e+00,  1.6246e-02,\n",
      "        -4.9306e-03, -1.3997e-02,  1.2811e-04, -7.5547e-03,  0.0000e+00,\n",
      "        -2.7592e-03, -4.0586e-04,  0.0000e+00,  0.0000e+00,  1.3879e-02,\n",
      "        -3.5085e-03,  0.0000e+00, -1.7714e-03,  3.3385e-04, -4.3566e-03,\n",
      "        -3.0930e-03,  1.6988e-03,  9.9267e-03])\n",
      "Parameter: torch.Size([4, 128]), Gradient: tensor([[ 0.0000e+00, -4.5334e-02, -5.4903e-02, -8.7673e-03, -4.2555e-02,\n",
      "         -2.7733e-02, -2.0376e-02, -6.7539e-04, -1.4306e-04, -2.8954e-04,\n",
      "         -1.7121e-03, -3.4355e-03,  0.0000e+00, -6.4528e-03, -1.5348e-02,\n",
      "         -2.5102e-02, -6.1392e-02, -4.1734e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -2.6299e-02,  0.0000e+00, -4.6604e-02, -4.4201e-04, -1.6710e-02,\n",
      "         -9.3294e-03,  0.0000e+00,  0.0000e+00, -3.0736e-03,  0.0000e+00,\n",
      "         -3.0295e-02, -7.2201e-04, -4.6991e-03, -1.1977e-02, -2.6110e-03,\n",
      "         -1.2183e-02, -1.1392e-02, -5.1274e-02, -7.0838e-03,  0.0000e+00,\n",
      "         -3.8707e-02, -4.5069e-03,  0.0000e+00, -1.8511e-02, -2.2657e-02,\n",
      "         -7.3150e-03, -3.4755e-03, -3.1446e-02,  0.0000e+00, -1.8190e-03,\n",
      "         -4.4273e-02,  0.0000e+00, -1.0270e-02, -6.8960e-04, -7.2349e-04,\n",
      "         -1.2704e-02, -1.1841e-03,  0.0000e+00, -2.1676e-02, -1.5835e-02,\n",
      "         -7.6087e-03, -1.1864e-02, -4.0261e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -5.4320e-03, -4.9304e-03,  0.0000e+00, -1.9674e-03, -2.3269e-02,\n",
      "         -6.3119e-03, -1.9458e-02, -6.4109e-03, -1.4398e-03,  0.0000e+00,\n",
      "         -3.2585e-03,  0.0000e+00, -2.9439e-03,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -3.4227e-02, -5.0796e-02, -3.0806e-02, -4.8387e-02,\n",
      "         -1.7719e-05,  0.0000e+00, -6.1805e-02, -4.3666e-02, -1.6693e-04,\n",
      "         -3.9099e-03, -2.0418e-04, -6.0424e-04, -5.1699e-04,  0.0000e+00,\n",
      "          0.0000e+00, -5.5507e-02,  0.0000e+00, -8.1857e-03, -1.5709e-02,\n",
      "         -5.3434e-03, -7.3563e-03, -3.7411e-02, -4.3775e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -2.7490e-02,  0.0000e+00, -1.1166e-02,\n",
      "         -1.6360e-02, -1.4330e-02, -6.7832e-05, -3.7929e-02,  0.0000e+00,\n",
      "         -4.4047e-04, -4.4981e-03,  0.0000e+00,  0.0000e+00, -2.0019e-02,\n",
      "         -1.8382e-04,  0.0000e+00, -3.5212e-03, -8.0022e-04, -6.1837e-04,\n",
      "         -7.5347e-03, -4.4952e-02, -3.1894e-02],\n",
      "        [ 0.0000e+00,  4.7982e-02,  1.2202e-01,  2.5518e-02,  1.1458e-01,\n",
      "         -1.1544e-03,  2.0332e-02,  2.3571e-03, -1.5315e-04, -3.0997e-04,\n",
      "         -1.8329e-03, -3.7760e-03,  0.0000e+00, -7.0923e-03,  2.2608e-02,\n",
      "          8.5163e-02,  1.6312e-01,  1.3374e-01,  0.0000e+00,  0.0000e+00,\n",
      "          1.6215e-02,  0.0000e+00,  7.3242e-02, -4.7319e-04,  4.0179e-02,\n",
      "          2.6873e-02,  0.0000e+00,  0.0000e+00,  1.0064e-02,  0.0000e+00,\n",
      "          7.9386e-02, -7.9356e-04, -5.1478e-03,  3.8510e-02, -2.8328e-03,\n",
      "          1.8691e-02, -1.2492e-02,  1.3223e-01,  1.5018e-02,  0.0000e+00,\n",
      "          9.1995e-02, -4.9536e-03,  0.0000e+00, -1.6918e-03,  5.7094e-02,\n",
      "         -8.0398e-03, -3.8147e-03,  1.0743e-01,  0.0000e+00, -1.9473e-03,\n",
      "          1.3103e-01,  0.0000e+00,  3.0849e-02, -7.3825e-04, -7.7453e-04,\n",
      "         -1.3962e-02, -1.3014e-03,  0.0000e+00,  6.9307e-03,  4.9902e-02,\n",
      "          2.1225e-02,  3.6165e-02,  1.1690e-01,  0.0000e+00,  0.0000e+00,\n",
      "          1.7616e-02,  1.4154e-02,  0.0000e+00, -2.1624e-03,  7.8740e-02,\n",
      "         -6.9374e-03,  6.5340e-02,  1.8994e-02, -1.2508e-03,  0.0000e+00,\n",
      "         -3.5621e-03,  0.0000e+00, -3.2356e-03,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  1.0556e-01,  1.6318e-01,  3.4765e-02,  1.1519e-01,\n",
      "         -1.8969e-05,  0.0000e+00,  1.4389e-01,  1.0823e-01, -1.7871e-04,\n",
      "         -4.2974e-03, -2.1858e-04, -6.4687e-04, -5.6822e-04,  0.0000e+00,\n",
      "          0.0000e+00,  1.8337e-01,  0.0000e+00, -8.9969e-03,  2.1628e-02,\n",
      "          1.4973e-02, -8.0853e-03,  7.2050e-02,  1.5277e-02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  3.8268e-02,  0.0000e+00,  2.5932e-02,\n",
      "         -1.7975e-02,  1.5371e-02, -7.2618e-05,  4.2623e-02,  0.0000e+00,\n",
      "         -4.8412e-04, -4.9356e-03,  0.0000e+00,  0.0000e+00,  6.6487e-02,\n",
      "         -2.0203e-04,  0.0000e+00, -3.8701e-03, -8.5668e-04, -6.7965e-04,\n",
      "          2.6295e-02,  1.3627e-01,  1.0242e-01],\n",
      "        [ 0.0000e+00,  4.3190e-02, -1.4466e-02, -8.9458e-03, -3.2568e-02,\n",
      "          5.8619e-02,  2.0850e-02, -1.0746e-03,  4.1507e-04,  8.4007e-04,\n",
      "          4.9675e-03,  1.1073e-02,  0.0000e+00,  2.0798e-02,  8.0533e-03,\n",
      "         -3.7535e-02, -4.4125e-02, -5.4120e-02,  0.0000e+00,  0.0000e+00,\n",
      "          3.7428e-02,  0.0000e+00,  1.9593e-02,  1.2824e-03, -7.6862e-03,\n",
      "         -9.1322e-03,  0.0000e+00,  0.0000e+00, -4.2375e-03,  0.0000e+00,\n",
      "         -2.0985e-02,  2.3271e-03,  1.4955e-02, -1.5613e-02,  7.9995e-03,\n",
      "          5.6126e-03,  3.6388e-02, -3.2715e-02, -1.2426e-03,  0.0000e+00,\n",
      "         -1.6775e-02,  1.4526e-02,  0.0000e+00,  3.9769e-02, -1.3492e-02,\n",
      "          2.3576e-02,  1.1143e-02, -4.7751e-02,  0.0000e+00,  5.2775e-03,\n",
      "         -4.5837e-02,  0.0000e+00, -1.1423e-02,  2.0008e-03,  2.0991e-03,\n",
      "          4.0944e-02,  3.8162e-03,  0.0000e+00,  3.7463e-02, -1.9915e-02,\n",
      "         -6.8569e-03, -1.3715e-02, -3.9601e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -7.3214e-03, -4.8373e-03,  0.0000e+00,  6.3410e-03, -3.4594e-02,\n",
      "          2.0344e-02, -2.8432e-02, -6.8707e-03,  4.2925e-03,  0.0000e+00,\n",
      "          1.0284e-02,  0.0000e+00,  9.4883e-03,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -3.9887e-02, -6.6666e-02,  2.7255e-02, -2.0961e-02,\n",
      "          5.1410e-05,  0.0000e+00, -2.3321e-02, -2.3470e-02,  4.8433e-04,\n",
      "          1.2602e-02,  5.9240e-04,  1.7531e-03,  1.6663e-03,  0.0000e+00,\n",
      "          0.0000e+00, -7.8122e-02,  0.0000e+00,  2.6383e-02,  9.4163e-03,\n",
      "         -4.8817e-03,  2.3710e-02,  1.6908e-03, -6.9649e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.6761e-02,  0.0000e+00, -4.0904e-03,\n",
      "          5.2655e-02,  1.3447e-02,  1.9681e-04,  3.3446e-02,  0.0000e+00,\n",
      "          1.4196e-03,  1.4405e-02,  0.0000e+00,  0.0000e+00, -2.8526e-02,\n",
      "          5.9244e-04,  0.0000e+00,  1.1349e-02,  2.3217e-03,  1.9930e-03,\n",
      "         -1.1988e-02, -5.0299e-02, -4.1990e-02],\n",
      "        [ 0.0000e+00, -4.5838e-02, -5.2648e-02, -7.8046e-03, -3.9458e-02,\n",
      "         -2.9731e-02, -2.0805e-02, -6.0706e-04, -1.1886e-04, -2.4056e-04,\n",
      "         -1.4225e-03, -3.8613e-03,  0.0000e+00, -7.2526e-03, -1.5314e-02,\n",
      "         -2.2526e-02, -5.7604e-02, -3.7891e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -2.7345e-02,  0.0000e+00, -4.6231e-02, -3.6723e-04, -1.5782e-02,\n",
      "         -8.4109e-03,  0.0000e+00,  0.0000e+00, -2.7528e-03,  0.0000e+00,\n",
      "         -2.8107e-02, -8.1149e-04, -5.1077e-03, -1.0920e-02, -2.5557e-03,\n",
      "         -1.2120e-02, -1.2504e-02, -4.8236e-02, -6.6911e-03,  0.0000e+00,\n",
      "         -3.6513e-02, -5.0655e-03,  0.0000e+00, -1.9566e-02, -2.0946e-02,\n",
      "         -8.2215e-03, -3.8528e-03, -2.8230e-02,  0.0000e+00, -1.5112e-03,\n",
      "         -4.0918e-02,  0.0000e+00, -9.1562e-03, -5.7293e-04, -6.0109e-04,\n",
      "         -1.4278e-02, -1.3308e-03,  0.0000e+00, -2.2717e-02, -1.4153e-02,\n",
      "         -6.7594e-03, -1.0586e-02, -3.7036e-02,  0.0000e+00,  0.0000e+00,\n",
      "         -4.8624e-03, -4.3861e-03,  0.0000e+00, -2.2112e-03, -2.0878e-02,\n",
      "         -7.0942e-03, -1.7451e-02, -5.7119e-03, -1.6020e-03,  0.0000e+00,\n",
      "         -3.4634e-03,  0.0000e+00, -3.3087e-03,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -3.1446e-02, -4.5717e-02, -3.1214e-02, -4.5844e-02,\n",
      "         -1.4722e-05,  0.0000e+00, -5.8765e-02, -4.1089e-02, -1.3869e-04,\n",
      "         -4.3945e-03, -1.6964e-04, -5.0201e-04, -5.8106e-04,  0.0000e+00,\n",
      "          0.0000e+00, -4.9737e-02,  0.0000e+00, -9.2002e-03, -1.5335e-02,\n",
      "         -4.7480e-03, -8.2680e-03, -3.6329e-02, -3.9346e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -2.7538e-02,  0.0000e+00, -1.0676e-02,\n",
      "         -1.8320e-02, -1.4487e-02, -5.6356e-05, -3.8140e-02,  0.0000e+00,\n",
      "         -4.9506e-04, -4.9714e-03,  0.0000e+00,  0.0000e+00, -1.7943e-02,\n",
      "         -2.0660e-04,  0.0000e+00, -3.9576e-03, -6.6484e-04, -6.9501e-04,\n",
      "         -6.7724e-03, -4.1016e-02, -2.8534e-02]])\n",
      "Parameter: torch.Size([4]), Gradient: tensor([-0.0672,  0.1443, -0.0129, -0.0642])\n",
      "Parameter: torch.Size([128, 10]), Gradient: tensor([[ 0.0395, -0.0232, -0.0585,  ...,  0.0214,  0.0121, -0.0101],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0335,  0.0196,  0.0496,  ..., -0.0181, -0.0103,  0.0086],\n",
      "        ...,\n",
      "        [-0.0165,  0.0096,  0.0244,  ..., -0.0089, -0.0050,  0.0042],\n",
      "        [-0.0507,  0.0298,  0.0751,  ..., -0.0275, -0.0155,  0.0130],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Parameter: torch.Size([128]), Gradient: tensor([-3.2422e-02,  0.0000e+00,  2.7461e-02,  0.0000e+00,  6.9540e-04,\n",
      "        -4.2068e-02,  0.0000e+00,  1.3719e-02,  9.8333e-04,  5.0405e-03,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  3.1888e-03,  0.0000e+00,\n",
      "         3.6493e-02,  3.6295e-02,  2.5848e-02,  2.1077e-04,  0.0000e+00,\n",
      "         1.4123e-03,  1.0499e-04,  0.0000e+00,  0.0000e+00,  4.5403e-03,\n",
      "         0.0000e+00,  2.0454e-03, -2.1070e-03, -1.5536e-03, -8.2249e-04,\n",
      "         0.0000e+00,  1.0207e-02,  0.0000e+00,  3.9478e-04, -2.8265e-02,\n",
      "        -1.2512e-02, -2.3514e-04,  3.7462e-02, -3.6764e-03,  2.0202e-04,\n",
      "        -9.6133e-03,  8.7857e-03,  0.0000e+00,  0.0000e+00, -1.6796e-02,\n",
      "         0.0000e+00,  1.7697e-02,  0.0000e+00,  3.1546e-02,  0.0000e+00,\n",
      "        -2.4157e-03,  0.0000e+00,  3.2398e-02, -3.1610e-02,  1.8469e-02,\n",
      "        -1.4927e-03,  0.0000e+00,  4.4839e-03, -4.1023e-02, -6.7869e-04,\n",
      "        -3.0063e-02,  1.5174e-02,  4.9236e-03,  2.7544e-02, -2.9995e-02,\n",
      "         2.2660e-02, -1.3074e-02,  9.6811e-04,  2.6575e-03,  2.5057e-04,\n",
      "         5.2403e-02, -4.1370e-02,  6.2613e-05,  4.0962e-02, -1.0003e-03,\n",
      "         8.4713e-03,  0.0000e+00,  4.7891e-02,  9.9242e-03,  2.4811e-02,\n",
      "         5.2482e-04,  3.3913e-03, -3.3737e-03, -3.9463e-02,  1.9313e-04,\n",
      "         0.0000e+00,  2.1035e-03,  0.0000e+00,  3.3946e-02, -3.3272e-03,\n",
      "         0.0000e+00,  4.7938e-02, -8.7764e-03, -3.5580e-02,  1.6395e-02,\n",
      "        -4.0190e-02,  8.7855e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -9.4882e-04, -3.0537e-02,  2.1634e-02,  4.0469e-02,  1.8643e-02,\n",
      "         0.0000e+00, -4.6340e-02,  4.3206e-02,  7.6910e-05,  0.0000e+00,\n",
      "         3.3287e-02, -2.7589e-04,  1.4398e-03,  3.2619e-03,  3.0508e-02,\n",
      "         0.0000e+00,  0.0000e+00, -1.6158e-03, -1.2832e-02, -4.4437e-02,\n",
      "         0.0000e+00,  2.4235e-04,  0.0000e+00, -4.7474e-02, -1.8358e-02,\n",
      "         1.3489e-02,  4.1605e-02,  0.0000e+00])\n",
      "Parameter: torch.Size([1, 128]), Gradient: tensor([[2.8498e-01, 0.0000e+00, 3.0015e-01, 0.0000e+00, 4.4741e-03, 2.6691e-02,\n",
      "         0.0000e+00, 1.5557e-01, 1.6312e-02, 1.8018e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1442e-02, 0.0000e+00, 2.4379e-01, 4.6078e-01, 3.4227e-01,\n",
      "         1.2089e-02, 0.0000e+00, 7.9523e-03, 1.8011e-04, 0.0000e+00, 0.0000e+00,\n",
      "         1.0925e-04, 0.0000e+00, 2.9318e-03, 1.1068e-02, 1.1069e-02, 5.2283e-03,\n",
      "         0.0000e+00, 1.1966e-01, 0.0000e+00, 1.4581e-01, 2.1247e-01, 3.8231e-01,\n",
      "         2.8409e-03, 1.6287e-01, 8.7838e-03, 9.5824e-03, 2.5377e-01, 4.2309e-01,\n",
      "         0.0000e+00, 0.0000e+00, 3.2008e-01, 0.0000e+00, 1.3075e-01, 0.0000e+00,\n",
      "         2.6271e-01, 0.0000e+00, 7.1356e-02, 0.0000e+00, 2.2804e-01, 3.6420e-01,\n",
      "         2.0175e-01, 1.1086e-01, 0.0000e+00, 3.7463e-01, 2.0861e-02, 1.6685e-03,\n",
      "         1.8042e-01, 2.2758e-01, 9.0457e-02, 2.8789e-01, 6.4282e-01, 4.2240e-01,\n",
      "         3.2811e-01, 1.0458e-03, 3.4393e-01, 7.8607e-03, 4.3833e-02, 2.3273e-01,\n",
      "         4.1509e-03, 1.1059e-01, 1.4252e-03, 1.5608e-01, 0.0000e+00, 1.6013e-01,\n",
      "         2.5165e-01, 1.2275e-01, 9.3453e-03, 4.9997e-02, 3.6687e-02, 4.1388e-01,\n",
      "         3.2284e-03, 0.0000e+00, 3.5926e-03, 0.0000e+00, 5.1692e-01, 7.9721e-03,\n",
      "         0.0000e+00, 1.8244e-01, 1.8494e-01, 2.3750e-02, 3.9143e-01, 1.8421e-02,\n",
      "         1.9774e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4124e-02, 2.9394e-01,\n",
      "         2.4378e-01, 3.6506e-01, 1.1305e-01, 0.0000e+00, 3.2729e-01, 8.1585e-02,\n",
      "         7.3146e-04, 0.0000e+00, 6.7909e-01, 2.7984e-03, 4.0629e-03, 1.6377e-01,\n",
      "         5.2934e-01, 0.0000e+00, 0.0000e+00, 4.2574e-02, 5.3723e-01, 2.7898e-01,\n",
      "         0.0000e+00, 1.7298e-03, 0.0000e+00, 2.8999e-01, 5.8101e-02, 1.2572e-01,\n",
      "         2.1747e-01, 0.0000e+00]])\n",
      "Parameter: torch.Size([1]), Gradient: tensor([0.5971])\n",
      "----------------After Clipping-----------------\n",
      "Parameter: torch.Size([128, 10]), Gradient: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.4934e-03,  1.6912e-03,  3.6124e-03,  ..., -1.1302e-03,\n",
      "         -2.1269e-05,  6.6078e-04],\n",
      "        [-1.1288e-03,  6.3018e-04,  2.2376e-03,  ..., -7.7880e-04,\n",
      "         -1.2296e-03,  5.4314e-04],\n",
      "        ...,\n",
      "        [ 7.5170e-04, -4.9179e-04, -1.1674e-03,  ...,  3.7587e-04,\n",
      "          1.6636e-04, -2.3083e-04],\n",
      "        [ 5.9751e-05, -1.1392e-04,  2.8265e-04,  ..., -1.2826e-04,\n",
      "         -7.2085e-04,  1.3629e-04],\n",
      "        [-2.4134e-03,  1.5344e-03,  3.6791e-03,  ..., -1.2288e-03,\n",
      "         -5.5991e-04,  6.9732e-04]])\n",
      "Parameter: torch.Size([128]), Gradient: tensor([ 0.0000e+00,  1.7151e-03,  1.3980e-03,  3.3390e-03, -2.5724e-03,\n",
      "         1.8764e-03,  5.5761e-04,  1.8455e-04,  9.7351e-06, -8.9603e-05,\n",
      "         7.2940e-05,  2.4003e-04,  0.0000e+00,  3.4959e-04, -7.8382e-04,\n",
      "        -7.5312e-04, -7.2517e-04, -2.6515e-04,  0.0000e+00,  0.0000e+00,\n",
      "        -1.8610e-03,  0.0000e+00, -3.1377e-03, -8.9364e-05, -1.7729e-05,\n",
      "        -1.2881e-03,  0.0000e+00,  0.0000e+00,  1.4162e-03,  0.0000e+00,\n",
      "        -2.3185e-03, -6.1624e-04,  5.9143e-04, -1.3404e-04,  2.3372e-04,\n",
      "        -2.5650e-04, -9.4720e-04,  2.4455e-04, -9.7468e-04,  0.0000e+00,\n",
      "        -1.4345e-03, -6.9949e-04,  0.0000e+00,  5.5136e-04,  3.1529e-04,\n",
      "         1.6477e-04, -5.9597e-04,  3.3887e-03,  0.0000e+00,  3.8818e-05,\n",
      "        -2.4882e-03,  0.0000e+00,  2.6812e-03,  5.5006e-05,  1.0849e-04,\n",
      "        -1.3315e-03,  9.4637e-04,  0.0000e+00, -3.2432e-03, -8.8932e-04,\n",
      "        -1.1902e-03,  2.7992e-03, -1.1555e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -1.4423e-03, -2.7007e-03,  0.0000e+00,  1.1812e-03, -1.9955e-03,\n",
      "        -8.4174e-04, -6.9791e-04, -8.9877e-04, -8.1678e-04,  0.0000e+00,\n",
      "         8.5956e-04,  0.0000e+00,  5.2718e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -6.9871e-04,  9.7071e-04, -1.2755e-03,  2.2197e-03,\n",
      "        -1.3762e-05,  0.0000e+00, -7.5037e-04, -1.3418e-03,  8.8336e-05,\n",
      "        -5.2656e-04,  8.4688e-05, -4.7372e-05,  1.0594e-03,  0.0000e+00,\n",
      "         0.0000e+00,  9.2250e-04,  0.0000e+00, -6.5499e-04, -1.5460e-03,\n",
      "         2.2746e-03, -4.6249e-04,  2.5252e-03, -4.5395e-04,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.0285e-04,  0.0000e+00,  3.1432e-03,\n",
      "        -9.5397e-04, -2.7082e-03,  2.4787e-05, -1.4617e-03,  0.0000e+00,\n",
      "        -5.3385e-04, -7.8526e-05,  0.0000e+00,  0.0000e+00,  2.6854e-03,\n",
      "        -6.7883e-04,  0.0000e+00, -3.4273e-04,  6.4593e-05, -8.4291e-04,\n",
      "        -5.9844e-04,  3.2869e-04,  1.9206e-03])\n",
      "Parameter: torch.Size([4, 128]), Gradient: tensor([[ 0.0000e+00, -8.7711e-03, -1.0623e-02, -1.6963e-03, -8.2336e-03,\n",
      "         -5.3657e-03, -3.9423e-03, -1.3067e-04, -2.7679e-05, -5.6020e-05,\n",
      "         -3.3126e-04, -6.6471e-04,  0.0000e+00, -1.2485e-03, -2.9695e-03,\n",
      "         -4.8567e-03, -1.1878e-02, -8.0747e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -5.0882e-03,  0.0000e+00, -9.0169e-03, -8.5519e-05, -3.2331e-03,\n",
      "         -1.8051e-03,  0.0000e+00,  0.0000e+00, -5.9468e-04,  0.0000e+00,\n",
      "         -5.8614e-03, -1.3969e-04, -9.0918e-04, -2.3172e-03, -5.0517e-04,\n",
      "         -2.3572e-03, -2.2042e-03, -9.9205e-03, -1.3706e-03,  0.0000e+00,\n",
      "         -7.4891e-03, -8.7200e-04,  0.0000e+00, -3.5814e-03, -4.3836e-03,\n",
      "         -1.4153e-03, -6.7244e-04, -6.0841e-03,  0.0000e+00, -3.5193e-04,\n",
      "         -8.5658e-03,  0.0000e+00, -1.9870e-03, -1.3342e-04, -1.3998e-04,\n",
      "         -2.4579e-03, -2.2909e-04,  0.0000e+00, -4.1939e-03, -3.0636e-03,\n",
      "         -1.4721e-03, -2.2955e-03, -7.7897e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0510e-03, -9.5393e-04,  0.0000e+00, -3.8065e-04, -4.5020e-03,\n",
      "         -1.2212e-03, -3.7647e-03, -1.2404e-03, -2.7857e-04,  0.0000e+00,\n",
      "         -6.3046e-04,  0.0000e+00, -5.6958e-04,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -6.6223e-03, -9.8279e-03, -5.9603e-03, -9.3618e-03,\n",
      "         -3.4283e-06,  0.0000e+00, -1.1958e-02, -8.4484e-03, -3.2297e-05,\n",
      "         -7.5649e-04, -3.9504e-05, -1.1691e-04, -1.0003e-04,  0.0000e+00,\n",
      "          0.0000e+00, -1.0739e-02,  0.0000e+00, -1.5838e-03, -3.0394e-03,\n",
      "         -1.0338e-03, -1.4233e-03, -7.2383e-03, -8.4695e-04,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -5.3188e-03,  0.0000e+00, -2.1604e-03,\n",
      "         -3.1653e-03, -2.7726e-03, -1.3124e-05, -7.3384e-03,  0.0000e+00,\n",
      "         -8.5221e-05, -8.7028e-04,  0.0000e+00,  0.0000e+00, -3.8732e-03,\n",
      "         -3.5565e-05,  0.0000e+00, -6.8127e-04, -1.5483e-04, -1.1964e-04,\n",
      "         -1.4578e-03, -8.6974e-03, -6.1707e-03],\n",
      "        [ 0.0000e+00,  9.2836e-03,  2.3608e-02,  4.9371e-03,  2.2169e-02,\n",
      "         -2.2335e-04,  3.9337e-03,  4.5604e-04, -2.9632e-05, -5.9973e-05,\n",
      "         -3.5463e-04, -7.3057e-04,  0.0000e+00, -1.3722e-03,  4.3742e-03,\n",
      "          1.6477e-02,  3.1560e-02,  2.5877e-02,  0.0000e+00,  0.0000e+00,\n",
      "          3.1372e-03,  0.0000e+00,  1.4171e-02, -9.1553e-05,  7.7737e-03,\n",
      "          5.1993e-03,  0.0000e+00,  0.0000e+00,  1.9472e-03,  0.0000e+00,\n",
      "          1.5360e-02, -1.5354e-04, -9.9600e-04,  7.4508e-03, -5.4809e-04,\n",
      "          3.6162e-03, -2.4169e-03,  2.5583e-02,  2.9056e-03,  0.0000e+00,\n",
      "          1.7799e-02, -9.5841e-04,  0.0000e+00, -3.2733e-04,  1.1046e-02,\n",
      "         -1.5555e-03, -7.3806e-04,  2.0785e-02,  0.0000e+00, -3.7676e-04,\n",
      "          2.5351e-02,  0.0000e+00,  5.9686e-03, -1.4284e-04, -1.4986e-04,\n",
      "         -2.7014e-03, -2.5179e-04,  0.0000e+00,  1.3409e-03,  9.6549e-03,\n",
      "          4.1066e-03,  6.9972e-03,  2.2617e-02,  0.0000e+00,  0.0000e+00,\n",
      "          3.4083e-03,  2.7385e-03,  0.0000e+00, -4.1837e-04,  1.5235e-02,\n",
      "         -1.3422e-03,  1.2642e-02,  3.6748e-03, -2.4200e-04,  0.0000e+00,\n",
      "         -6.8919e-04,  0.0000e+00, -6.2602e-04,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  2.0424e-02,  3.1572e-02,  6.7263e-03,  2.2287e-02,\n",
      "         -3.6702e-06,  0.0000e+00,  2.7840e-02,  2.0939e-02, -3.4576e-05,\n",
      "         -8.3146e-04, -4.2291e-05, -1.2515e-04, -1.0994e-04,  0.0000e+00,\n",
      "          0.0000e+00,  3.5477e-02,  0.0000e+00, -1.7407e-03,  4.1846e-03,\n",
      "          2.8970e-03, -1.5643e-03,  1.3940e-02,  2.9558e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  7.4040e-03,  0.0000e+00,  5.0174e-03,\n",
      "         -3.4777e-03,  2.9739e-03, -1.4050e-05,  8.2467e-03,  0.0000e+00,\n",
      "         -9.3666e-05, -9.5493e-04,  0.0000e+00,  0.0000e+00,  1.2864e-02,\n",
      "         -3.9089e-05,  0.0000e+00, -7.4878e-04, -1.6575e-04, -1.3150e-04,\n",
      "          5.0876e-03,  2.6365e-02,  1.9816e-02],\n",
      "        [ 0.0000e+00,  8.3563e-03, -2.7989e-03, -1.7308e-03, -6.3012e-03,\n",
      "          1.1341e-02,  4.0340e-03, -2.0791e-04,  8.0307e-05,  1.6254e-04,\n",
      "          9.6111e-04,  2.1424e-03,  0.0000e+00,  4.0239e-03,  1.5581e-03,\n",
      "         -7.2623e-03, -8.5372e-03, -1.0471e-02,  0.0000e+00,  0.0000e+00,\n",
      "          7.2416e-03,  0.0000e+00,  3.7908e-03,  2.4812e-04, -1.4871e-03,\n",
      "         -1.7669e-03,  0.0000e+00,  0.0000e+00, -8.1988e-04,  0.0000e+00,\n",
      "         -4.0601e-03,  4.5024e-04,  2.8934e-03, -3.0208e-03,  1.5477e-03,\n",
      "          1.0859e-03,  7.0403e-03, -6.3297e-03, -2.4042e-04,  0.0000e+00,\n",
      "         -3.2456e-03,  2.8105e-03,  0.0000e+00,  7.6944e-03, -2.6103e-03,\n",
      "          4.5615e-03,  2.1559e-03, -9.2389e-03,  0.0000e+00,  1.0211e-03,\n",
      "         -8.8685e-03,  0.0000e+00, -2.2101e-03,  3.8711e-04,  4.0613e-04,\n",
      "          7.9218e-03,  7.3836e-04,  0.0000e+00,  7.2482e-03, -3.8531e-03,\n",
      "         -1.3267e-03, -2.6535e-03, -7.6620e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -1.4165e-03, -9.3591e-04,  0.0000e+00,  1.2268e-03, -6.6932e-03,\n",
      "          3.9360e-03, -5.5009e-03, -1.3293e-03,  8.3051e-04,  0.0000e+00,\n",
      "          1.9897e-03,  0.0000e+00,  1.8358e-03,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -7.7174e-03, -1.2898e-02,  5.2733e-03, -4.0556e-03,\n",
      "          9.9468e-06,  0.0000e+00, -4.5120e-03, -4.5410e-03,  9.3707e-05,\n",
      "          2.4382e-03,  1.1462e-04,  3.3919e-04,  3.2239e-04,  0.0000e+00,\n",
      "          0.0000e+00, -1.5115e-02,  0.0000e+00,  5.1045e-03,  1.8219e-03,\n",
      "         -9.4451e-04,  4.5873e-03,  3.2713e-04, -1.3476e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  3.2428e-03,  0.0000e+00, -7.9140e-04,\n",
      "          1.0188e-02,  2.6017e-03,  3.8078e-05,  6.4711e-03,  0.0000e+00,\n",
      "          2.7467e-04,  2.7871e-03,  0.0000e+00,  0.0000e+00, -5.5191e-03,\n",
      "          1.1463e-04,  0.0000e+00,  2.1958e-03,  4.4921e-04,  3.8561e-04,\n",
      "         -2.3195e-03, -9.7318e-03, -8.1242e-03],\n",
      "        [ 0.0000e+00, -8.8688e-03, -1.0186e-02, -1.5100e-03, -7.6343e-03,\n",
      "         -5.7524e-03, -4.0254e-03, -1.1745e-04, -2.2996e-05, -4.6543e-05,\n",
      "         -2.7522e-04, -7.4709e-04,  0.0000e+00, -1.4032e-03, -2.9629e-03,\n",
      "         -4.3583e-03, -1.1145e-02, -7.3310e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -5.2906e-03,  0.0000e+00, -8.9447e-03, -7.1052e-05, -3.0535e-03,\n",
      "         -1.6273e-03,  0.0000e+00,  0.0000e+00, -5.3260e-04,  0.0000e+00,\n",
      "         -5.4380e-03, -1.5701e-04, -9.8823e-04, -2.1127e-03, -4.9447e-04,\n",
      "         -2.3449e-03, -2.4192e-03, -9.3327e-03, -1.2946e-03,  0.0000e+00,\n",
      "         -7.0644e-03, -9.8007e-04,  0.0000e+00, -3.7857e-03, -4.0526e-03,\n",
      "         -1.5907e-03, -7.4544e-04, -5.4618e-03,  0.0000e+00, -2.9239e-04,\n",
      "         -7.9169e-03,  0.0000e+00, -1.7715e-03, -1.1085e-04, -1.1630e-04,\n",
      "         -2.7625e-03, -2.5748e-04,  0.0000e+00, -4.3953e-03, -2.7382e-03,\n",
      "         -1.3078e-03, -2.0481e-03, -7.1656e-03,  0.0000e+00,  0.0000e+00,\n",
      "         -9.4078e-04, -8.4861e-04,  0.0000e+00, -4.2783e-04, -4.0394e-03,\n",
      "         -1.3726e-03, -3.3764e-03, -1.1051e-03, -3.0995e-04,  0.0000e+00,\n",
      "         -6.7010e-04,  0.0000e+00, -6.4017e-04,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -6.0841e-03, -8.8453e-03, -6.0393e-03, -8.8699e-03,\n",
      "         -2.8483e-06,  0.0000e+00, -1.1370e-02, -7.9499e-03, -2.6833e-05,\n",
      "         -8.5025e-04, -3.2821e-05, -9.7129e-05, -1.1242e-04,  0.0000e+00,\n",
      "          0.0000e+00, -9.6230e-03,  0.0000e+00, -1.7800e-03, -2.9671e-03,\n",
      "         -9.1864e-04, -1.5997e-03, -7.0290e-03, -7.6126e-04,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -5.3280e-03,  0.0000e+00, -2.0656e-03,\n",
      "         -3.5446e-03, -2.8030e-03, -1.0904e-05, -7.3793e-03,  0.0000e+00,\n",
      "         -9.5783e-05, -9.6186e-04,  0.0000e+00,  0.0000e+00, -3.4716e-03,\n",
      "         -3.9972e-05,  0.0000e+00, -7.6570e-04, -1.2863e-04, -1.3447e-04,\n",
      "         -1.3103e-03, -7.9358e-03, -5.5208e-03]])\n",
      "Parameter: torch.Size([4]), Gradient: tensor([-0.0130,  0.0279, -0.0025, -0.0124])\n",
      "Parameter: torch.Size([128, 10]), Gradient: tensor([[ 0.0077, -0.0045, -0.0113,  ...,  0.0041,  0.0023, -0.0020],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0065,  0.0038,  0.0096,  ..., -0.0035, -0.0020,  0.0017],\n",
      "        ...,\n",
      "        [-0.0032,  0.0019,  0.0047,  ..., -0.0017, -0.0010,  0.0008],\n",
      "        [-0.0098,  0.0058,  0.0145,  ..., -0.0053, -0.0030,  0.0025],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Parameter: torch.Size([128]), Gradient: tensor([-6.2729e-03,  0.0000e+00,  5.3131e-03,  0.0000e+00,  1.3455e-04,\n",
      "        -8.1393e-03,  0.0000e+00,  2.6544e-03,  1.9025e-04,  9.7524e-04,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1696e-04,  0.0000e+00,\n",
      "         7.0607e-03,  7.0222e-03,  5.0011e-03,  4.0779e-05,  0.0000e+00,\n",
      "         2.7324e-04,  2.0314e-05,  0.0000e+00,  0.0000e+00,  8.7846e-04,\n",
      "         0.0000e+00,  3.9574e-04, -4.0765e-04, -3.0058e-04, -1.5913e-04,\n",
      "         0.0000e+00,  1.9748e-03,  0.0000e+00,  7.6381e-05, -5.4688e-03,\n",
      "        -2.4208e-03, -4.5495e-05,  7.2480e-03, -7.1130e-04,  3.9087e-05,\n",
      "        -1.8600e-03,  1.6998e-03,  0.0000e+00,  0.0000e+00, -3.2497e-03,\n",
      "         0.0000e+00,  3.4241e-03,  0.0000e+00,  6.1035e-03,  0.0000e+00,\n",
      "        -4.6739e-04,  0.0000e+00,  6.2683e-03, -6.1159e-03,  3.5733e-03,\n",
      "        -2.8880e-04,  0.0000e+00,  8.6755e-04, -7.9372e-03, -1.3131e-04,\n",
      "        -5.8165e-03,  2.9359e-03,  9.5261e-04,  5.3292e-03, -5.8035e-03,\n",
      "         4.3841e-03, -2.5295e-03,  1.8731e-04,  5.1417e-04,  4.8480e-05,\n",
      "         1.0139e-02, -8.0042e-03,  1.2114e-05,  7.9254e-03, -1.9353e-04,\n",
      "         1.6390e-03,  0.0000e+00,  9.2660e-03,  1.9201e-03,  4.8004e-03,\n",
      "         1.0154e-04,  6.5615e-04, -6.5274e-04, -7.6353e-03,  3.7367e-05,\n",
      "         0.0000e+00,  4.0699e-04,  0.0000e+00,  6.5679e-03, -6.4374e-04,\n",
      "         0.0000e+00,  9.2750e-03, -1.6980e-03, -6.8841e-03,  3.1720e-03,\n",
      "        -7.7760e-03,  1.6998e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.8358e-04, -5.9082e-03,  4.1858e-03,  7.8300e-03,  3.6071e-03,\n",
      "         0.0000e+00, -8.9658e-03,  8.3594e-03,  1.4881e-05,  0.0000e+00,\n",
      "         6.4404e-03, -5.3378e-05,  2.7858e-04,  6.3110e-04,  5.9026e-03,\n",
      "         0.0000e+00,  0.0000e+00, -3.1263e-04, -2.4827e-03, -8.5976e-03,\n",
      "         0.0000e+00,  4.6890e-05,  0.0000e+00, -9.1853e-03, -3.5520e-03,\n",
      "         2.6098e-03,  8.0498e-03,  0.0000e+00])\n",
      "Parameter: torch.Size([1, 128]), Gradient: tensor([[5.5138e-02, 0.0000e+00, 5.8073e-02, 0.0000e+00, 8.6564e-04, 5.1642e-03,\n",
      "         0.0000e+00, 3.0099e-02, 3.1560e-03, 3.4862e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.2137e-03, 0.0000e+00, 4.7169e-02, 8.9150e-02, 6.6222e-02,\n",
      "         2.3390e-03, 0.0000e+00, 1.5386e-03, 3.4847e-05, 0.0000e+00, 0.0000e+00,\n",
      "         2.1138e-05, 0.0000e+00, 5.6724e-04, 2.1414e-03, 2.1417e-03, 1.0116e-03,\n",
      "         0.0000e+00, 2.3152e-02, 0.0000e+00, 2.8211e-02, 4.1108e-02, 7.3968e-02,\n",
      "         5.4965e-04, 3.1512e-02, 1.6995e-03, 1.8540e-03, 4.9100e-02, 8.1859e-02,\n",
      "         0.0000e+00, 0.0000e+00, 6.1929e-02, 0.0000e+00, 2.5297e-02, 0.0000e+00,\n",
      "         5.0828e-02, 0.0000e+00, 1.3806e-02, 0.0000e+00, 4.4122e-02, 7.0465e-02,\n",
      "         3.9034e-02, 2.1450e-02, 0.0000e+00, 7.2483e-02, 4.0362e-03, 3.2282e-04,\n",
      "         3.4908e-02, 4.4031e-02, 1.7502e-02, 5.5700e-02, 1.2437e-01, 8.1726e-02,\n",
      "         6.3482e-02, 2.0234e-04, 6.6543e-02, 1.5209e-03, 8.4807e-03, 4.5028e-02,\n",
      "         8.0311e-04, 2.1396e-02, 2.7574e-04, 3.0197e-02, 0.0000e+00, 3.0982e-02,\n",
      "         4.8689e-02, 2.3750e-02, 1.8081e-03, 9.6734e-03, 7.0981e-03, 8.0076e-02,\n",
      "         6.2463e-04, 0.0000e+00, 6.9509e-04, 0.0000e+00, 1.0001e-01, 1.5424e-03,\n",
      "         0.0000e+00, 3.5299e-02, 3.5782e-02, 4.5951e-03, 7.5734e-02, 3.5641e-03,\n",
      "         3.8258e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7327e-03, 5.6871e-02,\n",
      "         4.7167e-02, 7.0631e-02, 2.1872e-02, 0.0000e+00, 6.3324e-02, 1.5785e-02,\n",
      "         1.4152e-04, 0.0000e+00, 1.3139e-01, 5.4144e-04, 7.8609e-04, 3.1686e-02,\n",
      "         1.0242e-01, 0.0000e+00, 0.0000e+00, 8.2372e-03, 1.0394e-01, 5.3976e-02,\n",
      "         0.0000e+00, 3.3468e-04, 0.0000e+00, 5.6107e-02, 1.1241e-02, 2.4325e-02,\n",
      "         4.2075e-02, 0.0000e+00]])\n",
      "Parameter: torch.Size([1]), Gradient: tensor([0.1155])\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "batchSize = 3\n",
    "inputLayer =10\n",
    "numOfActions = 4\n",
    "#model = SeparateActorCritic(inputLayer,numOfActions)\n",
    "net = SeparateActorCritic(inputLayer,numOfActions) #1a\n",
    "#net= SeparateActorCritic(inputLayer,gym.spaces.Discrete(numOfActions)) #1b\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "#Dummy values \n",
    "observations = torch.randn(batchSize,inputLayer)\n",
    "#Foward pass\n",
    "actionProbabilities, stateValues = net(observations)\n",
    "#Dummy values\n",
    "distibution = torch.distributions.Categorical(actionProbabilities)\n",
    "actions = distibution.sample()\n",
    "logProbabilities = distibution.log_prob(actions)\n",
    "entropies = distibution.entropy()\n",
    "#Dummy values\n",
    "\n",
    "returns = torch.randn(batchSize, requires_grad=True)\n",
    "#Advantage\n",
    "advantage = returns-stateValues.detach()\n",
    "#Actor loss\n",
    "actorLoss = -(logProbabilities * advantage).mean()-0.01 * entropies.mean()\n",
    "#Critic loss\n",
    "criterion = nn.MSELoss()\n",
    "criticLoss = criterion(stateValues.squeeze(-1), returns)\n",
    "#Total loss\n",
    "totalLoss = actorLoss + criticLoss\n",
    "#Backpropagation step\n",
    "optimizer.zero_grad()\n",
    "totalLoss.backward()   #gradient computation\n",
    "\n",
    "\n",
    "print(f\"----------------Before Clipping-----------------\")\n",
    "\n",
    "totalGradientBeforeClipping = torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=float('inf'))\n",
    "for parameter in net.parameters():\n",
    "  if parameter.grad is not None:\n",
    "    print(f\"Parameter: {parameter.shape}, Gradient: {parameter.grad}\")\n",
    "  else:\n",
    "    print(f\"Parameter: {parameter.shape}, Gradient: None\")\n",
    "#print(f\"Gradient before clipping: {totalGradientBeforeClipping}\")\n",
    "\n",
    "print(f\"----------------After Clipping-----------------\")\n",
    "\n",
    "totalGradientAfterClipping = torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "for parameter in net.parameters():\n",
    "  if parameter.grad is not None:\n",
    "    print(f\"Parameter: {parameter.shape}, Gradient: {parameter.grad}\")\n",
    "  else:\n",
    "    print(f\"Parameter: {parameter.shape}, Gradient: None\")\n",
    "#print(f\"Gradient after clipping: {totalGradientAfterClipping.item()}\")\n",
    "\n",
    "optimizer.step() #Optimizer step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952750fa74cd487",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: We use Gradient clipping to prevent the major issue of Gradient Explosion that happens during Backpropogation as Actor wants higher rewards and Critic demands accuracy - In GS, the gradient values become too large and we use the code given to have maximum gradient norm clipped to 0.5 and maintain its range. \n",
    "It is preferred to ensure our network updates the weights in a stable manner during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a9303f5a1c863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4cff31e6c6e7e4a",
   "metadata": {},
   "source": [
    "If you are working in a team, provide a contribution summary.\n",
    "| Team Member | Step# | Contribution (%) |\n",
    "|---|---|---|\n",
    "|   1  | Task 1 | 100  |\n",
    "|   2 | Task 2 | 100  |\n",
    "|   1 | Task 3 | 100  |\n",
    "|   2 | Task 4 | 100  |\n",
    "|   | **Total** |   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0a6e29f281e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0254baf5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
